{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dghu2txs4jfc"
      },
      "source": [
        "# Report: Solving Task Clarity for LLM-based Agents\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "The challenge of achieving task clarity for LLM-based Agents involves enabling these language model-based agents to understand and execute user-provided tasks accurately.\n",
        "This report explores a current solution and outlines future work in addressing this problem.\n",
        "Especially for multi-languages\n",
        "\n",
        "## Example:\n",
        "\n",
        "- **Client Input:** \"Find for me a product in a website.\"\n",
        "- **Expected Output:**\n",
        "  ```python\n",
        "  {'task_type': 'Information retrieval', 'scores': > 0.2, 'inference_time': < 10 s}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outline\n",
        "\n",
        "#### Sections:\n",
        "1. **Current Solution: Zero-Shot Classification on a Trained Bert-NLI on Multilingual Dataset**\n",
        "\n",
        "2. **Fine Tune**\n",
        "   - Focuses on the manual fine-tuning process, including adjustments in model parameters, architecture modifications, and specific training procedures.\n",
        "   \n",
        "     2.1. **Solution Approach**\n",
        "\n",
        "     2.2. **Dataset Preparation** \n",
        "\n",
        "     2.3. **Manual Fine Tuning**\n",
        "     \n",
        "     2.4. **Auto Fine Tuning**\n",
        "\n",
        "3. **Save, Load, and Test Original vs. Manual Fine Tune vs. Auto Fine Tune Model**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P6FwQmMJBhOm"
      },
      "source": [
        "## Current solution: Zero-Shot Classification on a trained Bert-NLI on Multilingual Dataset\n",
        "\n",
        "*   model_id: \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LGMnpLFmBqu8"
      },
      "outputs": [],
      "source": [
        "# #install library\n",
        "# ! pip install accelerate peft bitsandbytes guidance trl py7zr\n",
        "# ! pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaF_DU_taD3Z",
        "outputId": "212360b0-8c1e-4458-a540-e68b39166b54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "#logging Hugging Face\n",
        "hf_key = \"Your Hugging Face key\"\n",
        "!huggingface-cli login --token $hf_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR7Y1QaONt46",
        "outputId": "ba243e05-82fe-4070-d77f-315dcaefd54b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting optuna~=3.3.0\n",
            "  Downloading optuna-3.3.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna~=3.3.0)\n",
            "  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.10.0 (from optuna~=3.3.0)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Collecting colorlog (from optuna~=3.3.0)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna~=3.3.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna~=3.3.0) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna~=3.3.0) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna~=3.3.0) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna~=3.3.0) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna~=3.3.0)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna~=3.3.0) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna~=3.3.0) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna~=3.3.0) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.12.1 cmaes-0.10.0 colorlog-6.7.0 optuna-3.3.0\n"
          ]
        }
      ],
      "source": [
        "# install library\n",
        "!pip install transformers[sentencepiece]~=4.33.0 -qq\n",
        "!pip install datasets~=2.14.0 -qq\n",
        "!pip install accelerate~=0.23.0 -qq\n",
        "!pip install wandb~=0.15.0 -qq\n",
        "!pip install mdutils~=1.6.0 -qq\n",
        "!pip install scikit-learn~=1.2.0 -qq\n",
        "!pip install sentencepiece\n",
        "!pip install optuna~=3.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DrzNkEyBWkHc"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCcLnkbBMEwO",
        "outputId": "2f7edff0-8ff9-48d2-e43c-e8f82220ddd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NErV818CQl9",
        "outputId": "cbf988b6-e7d0-4840-97aa-8fa721969935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "task_type: Code generation, scores: 0.47, inference_time: 3.9\n"
          ]
        }
      ],
      "source": [
        "#experiments\n",
        "import asyncio\n",
        "import re\n",
        "import time\n",
        "\n",
        "input = \"Write a pong game in python.\"\n",
        "candidate_labels = [\"Greeting\", \"Information retrieval\", \"Sentiment analysis\", \"Text generation\", \"Code generation\", \"Q&A\", \"Summarization\", \"Translation\"]\n",
        "time_execution = time.time()\n",
        "output = classifier(input, candidate_labels, multi_label=False)\n",
        "time_execution = round(time.time() - time_execution, 2)\n",
        "print(f\"task_type: {output['labels'][0]}, scores: {round(output['scores'][0],2)}, inference_time: {time_execution}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LO7W5Vc-aq8s"
      },
      "outputs": [],
      "source": [
        "async def task_classifier(client_input: str, task_types: str):\n",
        "    \"\"\"Classify tasks for LLM-based gent\"\"\"\n",
        "    candidate_labels = [label.strip() for label in task_types.split(\",\")]\n",
        "    time_execution = time.time()\n",
        "    output = classifier(str(client_input), candidate_labels, multi_label=False)\n",
        "    # output = classifier(input, candidate_labels, multi_label=False)\n",
        "    time_execution = round(time.time() - time_execution, 2)\n",
        "    return {\"task_type\": output['labels'][0], \"scores\": round(output['scores'][0],2), \"inference_time\": time_execution}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PVjjXpeF-yv1"
      },
      "source": [
        "### Test the solution & results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ya24A2T-_Yh",
        "outputId": "8c473a40-c21c-4c39-9b1d-58deb8abb8bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: 'Summarize the latest scientific research.'\n",
            "Output: {'task_type': 'Summarization', 'scores': 0.97, 'inference_time': 2.33}\n",
            "\n",
            "Input: 'Translate this English text into French.'\n",
            "Output: {'task_type': 'Translation', 'scores': 0.92, 'inference_time': 3.87}\n",
            "\n",
            "Input: 'What's the weather forecast for tomorrow?'\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.92, 'inference_time': 7.62}\n",
            "\n",
            "Input: 'Generate Python code for a simple calculator.'\n",
            "Output: {'task_type': 'Code generation', 'scores': 0.98, 'inference_time': 6.1}\n",
            "\n",
            "Input: 'Please provide me with the most popular video currently trending on YouTube'\n",
            "Output: {'task_type': 'Translation', 'scores': 0.27, 'inference_time': 3.91}\n",
            "\n",
            "Input: 'Hãy dịch giúp mình'\n",
            "Output: {'task_type': 'Summarization', 'scores': 0.4, 'inference_time': 3.14}\n",
            "\n",
            "Input: 'Chào các bác'\n",
            "Output: {'task_type': 'Greeting', 'scores': 0.84, 'inference_time': 4.77}\n",
            "\n",
            "Input: 'Tìm kiếm thông tin giúp tôi'\n",
            "Output: {'task_type': 'Information retrieval', 'scores': 0.61, 'inference_time': 7.33}\n",
            "\n",
            "Input: '私に合った製品を見つける'\n",
            "Output: {'task_type': 'Translation', 'scores': 0.45, 'inference_time': 5.72}\n",
            "\n",
            "Input: '너무 사랑해요'\n",
            "Output: {'task_type': 'Sentiment analysis', 'scores': 0.27, 'inference_time': 4.8}\n",
            "\n",
            "Input: 'Рад встрече'\n",
            "Output: {'task_type': 'Greeting', 'scores': 0.26, 'inference_time': 5.14}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# List of test cases\n",
        "test_cases = [\n",
        "    \"Summarize the latest scientific research.\",\n",
        "    \"Translate this English text into French.\",\n",
        "    \"What's the weather forecast for tomorrow?\",\n",
        "    \"Generate Python code for a simple calculator.\",\n",
        "    \"Please provide me with the most popular video currently trending on YouTube\",\n",
        "    \"Hãy dịch giúp mình\",\n",
        "    \"Chào các bác\",\n",
        "    \"Tìm kiếm thông tin giúp tôi\",\n",
        "    \"私に合った製品を見つける\",\n",
        "    \"너무 사랑해요\",\n",
        "    \"Рад встрече\"\n",
        "]\n",
        "\n",
        "# Run the task_classifier function for each test case\n",
        "for test_case in test_cases:\n",
        "    result = await task_classifier(test_case, 'Greeting, Information retrieval, Sentiment analysis, Text generation, Code generation, Q&A, Summarization, Translation')\n",
        "    print(f\"Input: '{test_case}'\")\n",
        "    print(f\"Output: {result}\")\n",
        "    print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yvRWkDwP1oRN"
      },
      "source": [
        "## Finetune\n",
        "The solution is referred to Mr. Laurer's solution, available at [this GitHub link](https://github.com/MoritzLaurer/summer-school-transformers-2023/blob/main/4_tune_bert_nli.ipynb) for related content and examples.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution Approach\n",
        "\n",
        "Fine-tuning a Large Language Model (LLM) like DeBERTa for a task classification problem, where the task is to classify user queries into specific categories, is a nuanced process. The approach of transforming the dataset into a Natural Language Inference (NLI) format can be highly effective, especially when using a model pre-trained on NLI tasks like mDeBERTa-v3-base-mnli-xnli."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9y8ukvQi1wTf"
      },
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pFF3UAIviol",
        "outputId": "7f037d95-6c06-4cc3-beaf-a24a28134f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726,
          "referenced_widgets": [
            "5625b80df03e4ab19dd970f983b04777",
            "961a092682324bbb9b664e410d50ea0f",
            "802d91d2d17f440cb65b4e81e4db5efe",
            "c020e1635c2d48e698ab28f143605436",
            "d840e2c9d0fe4284b7b1b3ae9b4b3c99",
            "37bf329a11a84e6085299faadbc90e17",
            "eff7b9dac98d46a0b4c80b2a4ef027f1",
            "e88e528d3e4a46119f012503af71bb9a",
            "cca7064fd0bd4a57bc20b8913bc7d13d",
            "71f7703db6ec4b8cbecb339be23a94c1",
            "b8442770919d46b1a3ae5bd0fbeaede3",
            "1d12e5fbbc3740239a556903e2149ba7",
            "ea9281837f6f4abbbd4114206058d6a8",
            "3d515140dfc843b4af7ccc01690a2c50",
            "6a55388a041e4cc79edb4cb097e66eda",
            "8c1eb20063374289ac6637c958653239",
            "a32f69f0e59e4108b8daf42c0c273d27",
            "8c69739cc36a4663868ffc8ee7a4a016",
            "a382ddde21ed4ce89afa932cb14fd4e6",
            "9e4279c518304621a815875b339dc928",
            "9c4c0a98b37640cbb90b2a389b978396",
            "2603e3b46062490c8f5fabf01910ee9c"
          ]
        },
        "id": "7tgtP05_KYp9",
        "outputId": "7d747f86-b028-49b1-ac4a-8b31966d6b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Data:           Task                                            Premise  \\\n",
            "0  Translation  An LLM is used to translate a technical docume...   \n",
            "1  Translation  An LLM is tasked with initiating a conversatio...   \n",
            "2  Translation  A user queries an LLM for the latest research ...   \n",
            "3  Translation  An LLM analyzes customer reviews to determine ...   \n",
            "4  Translation  An LLM is used to write a short story in the s...   \n",
            "\n",
            "                                          Hypothesis       Label  \n",
            "0  LLMs can accurately translate text between lan...  Entailment  \n",
            "1  LLMs can accurately translate text between lan...  Entailment  \n",
            "2  LLMs can accurately translate text between lan...  Entailment  \n",
            "3  LLMs can accurately translate text between lan...  Entailment  \n",
            "4  LLMs can accurately translate text between lan...  Entailment  \n",
            "Train dataset:{'Task': Value(dtype='string', id=None), 'Premise': Value(dtype='string', id=None), 'Hypothesis': Value(dtype='string', id=None), 'Label': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), '__index_level_0__': Value(dtype='int64', id=None)}\n",
            "            label\n",
            "count  453.000000\n",
            "mean     0.750552\n",
            "std      0.821196\n",
            "min      0.000000\n",
            "25%      0.000000\n",
            "50%      1.000000\n",
            "75%      1.000000\n",
            "max      2.000000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5625b80df03e4ab19dd970f983b04777",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/453 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d12e5fbbc3740239a556903e2149ba7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/195 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The overall structure of the pre-processed train and test sets:\n",
            "\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Premise', 'Hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 453\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Premise', 'Hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 195\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# process data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datasets\n",
        "\n",
        "#dataset path\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/task_classification/LLM_NLI_Dataset_Balanced.csv'  # Replace with your transformed dataset path\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(dataset_path)\n",
        "print(f'Original Data: {df.head()}')\n",
        "\n",
        "# Numerical the labels\n",
        "label_mapping = {'Entailment': 0, 'Neutral': 1, 'Contradiction': 2}\n",
        "df['label'] = df['Label'].replace(label_mapping)\n",
        "\n",
        "\n",
        "# Split the dataset\n",
        "train, test = train_test_split(df, test_size=0.3, random_state=42) # 70% training, 30% for test\n",
        "\n",
        "# convert pandas dataframes to Hugging Face dataset object to facilitate pre-processing\n",
        "dataset = datasets.DatasetDict({\n",
        "    \"train\": datasets.Dataset.from_pandas(train),\n",
        "    \"test\": datasets.Dataset.from_pandas(test)\n",
        "})\n",
        "print(f'Train dataset:{dataset[\"train\"].features}')\n",
        "print(train.describe())\n",
        "\n",
        "## load the BERT-NLI's tokenizer\n",
        "# you can choose any of the NLI models here: https://huggingface.co/MoritzLaurer\n",
        "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"  # multilingual model: \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, model_max_length=512)\n",
        "\n",
        "# tokenize\n",
        "def tokenize_nli_format(examples):\n",
        "  return tokenizer(examples[\"Premise\"], examples[\"Hypothesis\"], truncation=True, max_length=512)  # max_length can be reduced to e.g. 256 to increase speed, but long texts will be cut off\n",
        "\n",
        "# Tokenize the dataset\n",
        "dataset = dataset.map(tokenize_nli_format, batched=True)\n",
        "\n",
        "# remove unnecessary columns for model training\n",
        "dataset = dataset.remove_columns([\n",
        "    'Task', '__index_level_0__', 'Label'])\n",
        "\n",
        "# inspect the dataset\n",
        "print(\"The overall structure of the pre-processed train and test sets:\\n\")\n",
        "print(dataset)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aqL0EkXpzeWZ"
      },
      "source": [
        "#### Manual Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjIZe8ZAQiR5",
        "outputId": "b59b6996-3acd-4fd5-b018-5b0dbe1ea0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "max_length = 512\n",
        "# Load the transformed NLI dataset\n",
        "max_length=512\n",
        "SEED_GLOBAL = 2023\n",
        "\n",
        "\n",
        "# Initialize the Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# use GPU (cuda) if available, otherwise use CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "model.to(device);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mnwd7gxKMnH9"
      },
      "source": [
        "##### Setting training arguments / hyperparameters, metrics\n",
        "\n",
        "The subsequent cell defines crucial hyperparameters. We selected settings that generally perform effectively to eliminate the necessity for hyperparameter exploration. Additionally, we offer code later on for conducting hyperparameter searches, allowing researchers the option to potentially enhance performance by a small margin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pCFbY9yuMywJ"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, logging\n",
        "\n",
        "# Set the directory to write the fine-tuned model and training logs to.\n",
        "# With google colab, this will create a temporary folder, which will be deleted once you disconnect.\n",
        "# You can connect to your personal google drive to save models and logs properly.\n",
        "training_directory = \"BERT-nli-demo\"\n",
        "\n",
        "# FP16 is a hyperparameter which can increase training speed and reduce memory consumption, but only on GPU and if batch-size > 8, see here: https://huggingface.co/transformers/performance.html?#fp16\n",
        "# FP16 does not work on CPU or for multilingual mDeBERTa models\n",
        "#fp16_bool = True if torch.cuda.is_available() else False\n",
        "#if \"mdeberta\" in model_name.lower(): fp16_bool = False  # multilingual mDeBERTa does not support FP16 yet: https://github.com/microsoft/DeBERTa/issues/77\n",
        "# in case of hyperparameter search end the end: FP16 has to be set to False. The integrated hyperparameter search with the Hugging Face Trainer can lead to errors otherwise.\n",
        "#fp16_bool = False\n",
        "\n",
        "# Hugging Face tipps to increase training speed and decrease out-of-memory (OOM) issues: https://huggingface.co/transformers/performance.html?\n",
        "# Overview of all training arguments: https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments\n",
        "train_args = TrainingArguments(\n",
        "    output_dir=f'./results/{training_directory}',\n",
        "    logging_dir=f'./logs/{training_directory}',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,  # if you get an out-of-memory error, reduce this value to 8 or 4 and restart the runtime. Higher values increase training speed, but also increase memory requirements. Ideal values here are always a multiple of 8.\n",
        "    per_device_eval_batch_size=80,  # if you get an out-of-memory error, reduce this value, e.g. to 40 and restart the runtime\n",
        "    #gradient_accumulation_steps=4, # Can be used in case of memory problems to reduce effective batch size. accumulates gradients over X steps, only then backward/update. decreases memory usage, but also slightly speed. (!adapt/halve batch size accordingly)\n",
        "    num_train_epochs=2,  # this can be increased, but higher values increase training time. Good values for NLI are between 3 and 20.\n",
        "    warmup_ratio=0.25,  # a good normal default value is 0.06 for normal BERT-base models, but since we want to reuse prior NLI knowledge and avoid catastrophic forgetting, we set the value higher\n",
        "    weight_decay=0.1,\n",
        "    seed=SEED_GLOBAL,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    #fp16=fp16_bool,  # Can speed up training and reduce memory consumption, but only makes sense at batch-size > 8. loads two copies of model weights, which creates overhead. https://huggingface.co/transformers/performance.html?#fp16\n",
        "    #fp16_full_eval=fp16_bool,\n",
        "    evaluation_strategy=\"epoch\", # options: \"no\"/\"steps\"/\"epoch\"\n",
        "    #eval_steps=10_000,  # evaluate after n steps if evaluation_strategy!='steps'. defaults to logging_steps\n",
        "    save_strategy = \"epoch\",  # options: \"no\"/\"steps\"/\"epoch\"\n",
        "    #save_steps=10_000,              # Number of updates steps before two checkpoint saves.\n",
        "    #save_total_limit=10,             # If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir\n",
        "    #logging_strategy=\"steps\",\n",
        "    report_to=[],  # \"all\"  # logging\n",
        "    #push_to_hub=False,\n",
        "    #push_to_hub_model_id=f\"{model_name}-finetuned-{task}\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OtDBxIpsjuwB"
      },
      "outputs": [],
      "source": [
        "# helper function to clean memory and reduce risk of out-of-memory error\n",
        "import gc\n",
        "def clean_memory():\n",
        "  #del(model)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "  gc.collect()\n",
        "\n",
        "clean_memory()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4UBumOT4NPI4"
      },
      "source": [
        "**Explanation of different training arguments:**\n",
        "\n",
        "You can find more arguments, explanations and examples in the Hugging Face [documentation](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments).\n",
        "\n",
        "* **num_train_epochs**:\n",
        "Specifies the number of times the entire training dataset is passed through the model. For example, num_train_epochs=3 means the trainer will iterate over the entire training dataset three times.\n",
        "\n",
        "* **per_device_train_batch_size**:\n",
        "The model does not learn from the entire dataset at once, but in batches of e.g. 16 texts. For example, if per_device_train_batch_size=16, then the model analyses 16 texts and sees how wrong it was on these 16 texts. After analysing these 16 texts, the model's parameters are updated/optimised to make the model less wrong on these texts. The degree to which the model's parameters are updated is called 'learning rate'.\n",
        "\n",
        "* **learning_rate**:\n",
        "The the \"rate\" or speed with which the model's parameters are updated by the optimisation algorithm. A smaller value makes the model's parameter updated more slowly after each batch, while a larger value updates the model's paramters more drastically. A good general value is 2e-5 (which means 0.00002).\n",
        "\n",
        "* **per_device_eval_batch_size**:\n",
        "The number of evaluation examples used in one batch during evaluation. This batch size is irrelevant for the model's learning. A higher value makes evaluation faster (more texts are processed at the same time), but higher values also cost memory and increase the risk of out-of-memory errors (OOM).\n",
        "\n",
        "* **gradient_accumulation_steps**:\n",
        "Indicates the number of steps before performing a backward/update pass. This means the loss is accumulated over gradient_accumulation_steps steps instead of updating after every step. Useful for training with larger effective batch sizes using limited memory.\n",
        "\n",
        "* **warmup_ratio**:\n",
        "Specifies the ratio of total training steps for which the learning rate will be linearly increased (warm-up phase) before it's decayed.\n",
        "\n",
        "* **weight_decay**:\n",
        "A regularization technique which penalizes large weights by adding a penalty term to the loss. It helps prevent overfitting.\n",
        "\n",
        "* **seed**:\n",
        "Sets a seed for reproducibility. Ensures that multiple runs with the same seed produce the same results.\n",
        "\n",
        "* **load_best_model_at_end**:\n",
        "Loads the best model (according to metric_for_best_model) at the end of training instead of the last model.\n",
        "\n",
        "* **metric_for_best_model**:\n",
        "Determines which metric to use for evaluating and determining the best model during training.\n",
        "\n",
        "* **evaluation_strategy**:\n",
        "Defines when to evaluate the model. For example, evaluation_strategy=\"epoch\" evaluates the model after every epoch.\n",
        "\n",
        "* **save_strategy**:\n",
        "Specifies when to save the model. For example, save_strategy=\"epoch\" saves the model after every epoch.\n",
        "\n",
        "* **fp16**:\n",
        "Enables mixed precision training if set to True, which can speed up training and reduce memory usage. But this does not work with every model and is only beneficial with a batch_size >= 16.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3kx-wJdN4sG",
        "outputId": "69a9f4f6-5039-4a8e-fd70-2661c29459a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['LLMs can accurately identify and classify sentiments in text, recognizing subtle emotional cues and expressions.'\n",
            " 'LLMs can accurately translate text between languages by understanding contextual nuances and idiomatic expressions, maintaining the original meaning and tone.'\n",
            " 'LLMs can create coherent, contextually appropriate, and engaging text across various genres and styles.'\n",
            " 'LLMs can curate and recommend content that aligns with user preferences and behaviors, enhancing user experience.'\n",
            " 'LLMs can efficiently retrieve relevant information from a vast corpus, understanding and interpreting complex user queries.'\n",
            " 'LLMs can generate appropriate and culturally sensitive greetings, adapting to various social contexts and user profiles.'\n",
            " 'LLMs can generate syntactically correct and logically coherent code, aiding in software development tasks.'\n",
            " 'LLMs can provide accurate, concise, and relevant answers to a wide range of questions, showcasing deep understanding and reasoning.'\n",
            " 'LLMs can sustain engaging, coherent, and contextually relevant conversations, simulating human-like interaction.'\n",
            " 'The LLM fails to detect sarcasm, resulting in inaccurate sentiment classification.'\n",
            " \"The LLM generates a story that is inconsistent with H.G. Wells' style.\"\n",
            " 'The LLM provides a general response that does not directly answer the question.'\n",
            " 'The LLM provides outdated information unrelated to quantum computing.'\n",
            " \"The LLM recommends movies that do not align with the user's demonstrated preferences.\"\n",
            " \"The LLM's greeting is generic and does not reflect cultural sensitivities.\"\n",
            " 'The conversation with the LLM is repetitive and lacks depth.'\n",
            " \"The document's technical jargon will remain untranslated due to its complexity.\"\n",
            " 'The generated code does not follow the requested specifications for data analysis.']\n"
          ]
        }
      ],
      "source": [
        "# Metrics\n",
        "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics_nli_binary(eval_pred, label_text_alphabetical=None):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    ### reformat model output to enable calculation of standard metrics\n",
        "    # split in chunks with predictions for each hypothesis for one unique premise\n",
        "    def chunks(lst, n):  # Yield successive n-sized chunks from lst. https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
        "        for i in range(0, len(lst), n):\n",
        "            yield lst[i:i + n]\n",
        "\n",
        "    # for each chunk/premise, select the most likely hypothesis\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "    prediction_chunks_lst = list(chunks(predictions, len(set(label_text_alphabetical)) ))\n",
        "    hypo_position_highest_prob = []\n",
        "    for i, chunk in enumerate(prediction_chunks_lst):\n",
        "        hypo_position_highest_prob.append(np.argmax(np.array(chunk)[:, 0]))  # only accesses the first column of the array, i.e. the entailment/true prediction logit of all hypos and takes the highest one\n",
        "\n",
        "    label_chunks_lst = list(chunks(labels, len(set(label_text_alphabetical)) ))\n",
        "    label_position_gold = []\n",
        "    for chunk in label_chunks_lst:\n",
        "        label_position_gold.append(np.argmin(chunk))  # argmin to detect the position of the 0 among the 1s\n",
        "\n",
        "    #print(\"Highest probability prediction per premise: \", hypo_position_highest_prob)\n",
        "    #print(\"Correct label per premise: \", label_position_gold)\n",
        "\n",
        "    ### calculate standard metrics\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(label_position_gold, hypo_position_highest_prob, average='macro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(label_position_gold, hypo_position_highest_prob, average='micro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
        "    acc_balanced = balanced_accuracy_score(label_position_gold, hypo_position_highest_prob)\n",
        "    acc_not_balanced = accuracy_score(label_position_gold, hypo_position_highest_prob)\n",
        "    metrics = {\n",
        "        'accuracy': acc_not_balanced,\n",
        "        'f1_macro': f1_macro,\n",
        "        'accuracy_balanced': acc_balanced,\n",
        "        'f1_micro': f1_micro,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'precision_micro': precision_micro,\n",
        "        'recall_micro': recall_micro,\n",
        "        #'label_gold_raw': label_position_gold,\n",
        "        #'label_predicted_raw': hypo_position_highest_prob\n",
        "    }\n",
        "    #print(\"Aggregate metrics: \", {key: metrics[key] for key in metrics if key not in [\"label_gold_raw\", \"label_predicted_raw\"]} )  # print metrics but without label lists\n",
        "    #print(\"Detailed metrics: \", classification_report(label_position_gold, hypo_position_highest_prob, labels=np.sort(pd.factorize(label_text_alphabetical, sort=True)[0]), target_names=label_text_alphabetical, sample_weight=None, digits=2, output_dict=True,\n",
        "    #                            zero_division='warn'), \"\\n\")\n",
        "    return metrics\n",
        "\n",
        "# Create alphabetically ordered list of the original dataset classes/labels\n",
        "# This is necessary to be sure that the ordering of the test set labels and predictions is the same. Otherwise there is a risk that labels and predictions are in a different order and resulting metrics are wrong.\n",
        "label_text_alphabetical = np.sort(train.Hypothesis.unique())\n",
        "print(label_text_alphabetical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "NL89LXzIzJKs",
        "outputId": "9d023cd8-6fc8-461d-cb7b-20f245c2765b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [58/58 01:08, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>Accuracy Balanced</th>\n",
              "      <th>F1 Micro</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>Precision Micro</th>\n",
              "      <th>Recall Micro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.924523</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.499559</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.041667</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=58, training_loss=1.1051797537968076, metrics={'train_runtime': 69.4023, 'train_samples_per_second': 13.054, 'train_steps_per_second': 0.836, 'total_flos': 26303077287630.0, 'train_loss': 1.1051797537968076, 'epoch': 2.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fine-tuning and evaluation\n",
        "\n",
        "## Fine-tuning\n",
        "# training\n",
        "manual_trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=train_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    compute_metrics=lambda eval_pred: compute_metrics_nli_binary(eval_pred, label_text_alphabetical=label_text_alphabetical)\n",
        ")\n",
        "\n",
        "manual_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "GkGvtLUEbD2x",
        "outputId": "e3e292d1-2a0d-4d8d-a5cc-30fe10f12ac7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual Fine-tuned Results:\n",
            " {'eval_loss': 0.4995587468147278, 'eval_accuracy': 0.09090909090909091, 'eval_f1_macro': 0.04166666666666667, 'eval_accuracy_balanced': 0.06666666666666667, 'eval_f1_micro': 0.09090909090909091, 'eval_precision_macro': 0.125, 'eval_recall_macro': 0.025, 'eval_precision_micro': 0.09090909090909091, 'eval_recall_micro': 0.09090909090909091, 'eval_runtime': 0.2833, 'eval_samples_per_second': 688.352, 'eval_steps_per_second': 10.59, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the fine-tuned model on the held-out test set\n",
        "results = manual_trainer.evaluate()\n",
        "print(f'Manual Fine-tuned Results:\\n {results}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Auto Fine Tune"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_UK6wKhMhh7Q"
      },
      "source": [
        "##### Automatically setting training arguments / hyperparameters\n",
        "- Enhance performance via hyperparameter search (hp-search) tailored to your task and dataset.\n",
        "- Conduct hp-search on a subset of the training set (e.g., validation set) to prevent test set data leakage.\n",
        "- Note: Small datasets might benefit from hp-search on two different train-validation splits (< 2000 data points).\n",
        "- See documentation on hp-search with Hugging Face Transformers [here](https://huggingface.co/docs/transformers/main/hpo_train).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYVt2K8mmstN",
        "outputId": "43e647e3-6d58-4abb-b98d-a22eea63b4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Premise', 'Hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 271\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Premise', 'Hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 182\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "## train-validation split - test set should not be visible during hp-search\n",
        "# https://huggingface.co/docs/datasets/v2.5.1/en/package_reference/main_classes#datasets.Dataset.train_test_split\n",
        "\n",
        "# the ideal size of the validation set depends on the size of your training data. Each label should have at the very least a few dozen examples in the validation set (ideally several hundred)\n",
        "validation_set_size = 0.4  # for a training data size of 1000 with 3 classes we use 40% of the training data for validating hyperparameters\n",
        "\n",
        "# train-validation split for hp-search\n",
        "dataset_hp = dataset[\"train\"].train_test_split(test_size=validation_set_size, seed=SEED_GLOBAL, shuffle=True)\n",
        "print(dataset_hp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4XT9YxePnjXJ"
      },
      "outputs": [],
      "source": [
        "# Clean memory and reduce risk of out-of-memory error\n",
        "clean_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2CsKMydsn3aA"
      },
      "outputs": [],
      "source": [
        "## Reinitialize trainer for hp-search\n",
        "# https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10\n",
        "\n",
        "def model_init():\n",
        "  clean_memory()\n",
        "  return AutoModelForSequenceClassification.from_pretrained(model_name).to(device)  # return_dict=True\n",
        "\n",
        "auto_trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    tokenizer=tokenizer,\n",
        "    args=train_args,\n",
        "    train_dataset=dataset_hp[\"train\"],\n",
        "    eval_dataset=dataset_hp[\"test\"],\n",
        "    compute_metrics=lambda eval_pred: compute_metrics_nli_binary(eval_pred, label_text_alphabetical=label_text_alphabetical)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hU-rtaRLn_ID"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters you want to optimise\n",
        "# Use Optuna for hp-search: https://optuna.readthedocs.io/en/stable/\n",
        "def hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [9e-6, 2e-5, 4e-5]),\n",
        "        \"num_train_epochs\": 4, #trial.suggest_int(\"num_train_epochs\", 4, 24, log=False, step=4),   # increasing the maximum number of epochs here could increase performance but will take (much) longer to train\n",
        "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.1, 0.6, log=True),\n",
        "        \"per_device_train_batch_size\": 16,  # lower this value in case of out-of-memory errors and restart the runtime\n",
        "        #\"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
        "        \"evaluation_strategy\": \"no\",\n",
        "        \"save_strategy\": \"no\",\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "AavTyMSyoJPv",
        "outputId": "412b17ec-0d56-4d70-af8c-e87a97bd52fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-11-17 05:51:09,934] A new study created in memory with name: no-name-684f2b85-2584-4b9b-b67d-49b92a6eee81\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [68/68 00:10, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:59]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
            "[I 2023-11-17 05:51:30,179] Trial 0 finished with value: 0.03703703703703704 and parameters: {'learning_rate': 2e-05, 'warmup_ratio': 0.12546162504661698}. Best is trial 0 with value: 0.03703703703703704.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [68/68 00:08, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
            "[I 2023-11-17 05:51:44,361] Trial 1 finished with value: 0.07333333333333333 and parameters: {'learning_rate': 2e-05, 'warmup_ratio': 0.36806941169521934}. Best is trial 1 with value: 0.07333333333333333.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [68/68 00:09, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
            "[I 2023-11-17 05:51:59,686] Trial 2 finished with value: 0.03333333333333334 and parameters: {'learning_rate': 2e-05, 'warmup_ratio': 0.24555638780055672}. Best is trial 1 with value: 0.07333333333333333.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [68/68 00:13, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
            "[I 2023-11-17 05:52:16,829] Trial 3 finished with value: 0.030303030303030307 and parameters: {'learning_rate': 9e-06, 'warmup_ratio': 0.5331240839812664}. Best is trial 1 with value: 0.07333333333333333.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [68/68 00:08, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
            "[I 2023-11-17 05:52:29,592] Trial 4 finished with value: 0.1111111111111111 and parameters: {'learning_rate': 4e-05, 'warmup_ratio': 0.4233655282316589}. Best is trial 4 with value: 0.1111111111111111.\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "\n",
        "# number of differen hp configurations to test\n",
        "numer_of_trials = 5  # increasing this value can lead to better hyperparameters, but will take longer\n",
        "# chose the sampler for sampling hp configurations\n",
        "optuna_sampler = optuna.samplers.TPESampler(\n",
        "    seed=SEED_GLOBAL, consider_prior=True, prior_weight=1.0, consider_magic_clip=True,\n",
        "    consider_endpoints=False, n_startup_trials=numer_of_trials/2, n_ei_candidates=24,\n",
        "    multivariate=False, group=False, warn_independent_sampling=True, constant_liar=False\n",
        ")  # https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.TPESampler.html#optuna.samplers.TPESampler\n",
        "\n",
        "# Hugging Face Documentation: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.hyperparameter_search\n",
        "best_run = auto_trainer.hyperparameter_search(\n",
        "    n_trials=numer_of_trials,\n",
        "    compute_objective=lambda metrics: metrics[\"eval_f1_macro\"],\n",
        "    direction=\"maximize\",\n",
        "    hp_space= hp_space,\n",
        "    backend='optuna',\n",
        "    **{\"sampler\": optuna_sampler}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skL6GA2Woe5d",
        "outputId": "7beca9f5-5659-41c1-e7d5-416cfdd50f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BestRun(run_id='4', objective=0.1111111111111111, hyperparameters={'learning_rate': 4e-05, 'warmup_ratio': 0.4233655282316589}, run_summary=None)\n"
          ]
        }
      ],
      "source": [
        "# show best hyperparameters based on hp-search\n",
        "print(best_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEFn8kZVpIur",
        "outputId": "d9668fea-bf0a-4ecb-b943-99fccb07d520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=4e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./logs/BERT-nli-demo,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1_macro,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=4,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=./results/BERT-nli-demo,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=80,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./results/BERT-nli-demo,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=no,\n",
            "save_total_limit=None,\n",
            "seed=2023,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.4233655282316589,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.1,\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# update the training arguments with the best hyperparameters\n",
        "for k,v in best_run.hyperparameters.items():\n",
        "    setattr(train_args, k, v)\n",
        "print(\"\\n\", train_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "V78EVFpapcOv",
        "outputId": "e5d845de-09a2-4dc7-9eef-40180d346455"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [116/116 00:15, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=116, training_loss=0.0957093567683779, metrics={'train_runtime': 15.9231, 'train_samples_per_second': 113.797, 'train_steps_per_second': 7.285, 'total_flos': 52510568689800.0, 'train_loss': 0.0957093567683779, 'epoch': 4.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training\n",
        "auto_trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=train_args,\n",
        "    train_dataset=dataset[\"train\"],  #.shard(index=1, num_shards=100),  # https://huggingface.co/docs/datasets/processing.html#sharding-the-dataset-shard\n",
        "    eval_dataset=dataset[\"test\"],  #.shard(index=1, num_shards=100),\n",
        "    compute_metrics=lambda eval_pred: compute_metrics_nli_binary(eval_pred, label_text_alphabetical=label_text_alphabetical)\n",
        ")\n",
        "\n",
        "auto_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "UIUrXqVMpejD",
        "outputId": "3640ffd1-59db-4693-8309-b94879ef9d97"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatical Fine-tuned Results:\n",
            " {'eval_loss': 0.00012210608110763133, 'eval_accuracy': 0.09090909090909091, 'eval_f1_macro': 0.044444444444444446, 'eval_accuracy_balanced': 0.08333333333333333, 'eval_f1_micro': 0.09090909090909091, 'eval_precision_macro': 0.1111111111111111, 'eval_recall_macro': 0.027777777777777776, 'eval_precision_micro': 0.09090909090909091, 'eval_recall_micro': 0.09090909090909091, 'eval_runtime': 0.3148, 'eval_samples_per_second': 619.499, 'eval_steps_per_second': 9.531, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
          ]
        }
      ],
      "source": [
        "## Evaluate the fine-tuned model on the held-out test set\n",
        "results = auto_trainer.evaluate()\n",
        "print(f'Automatical Fine-tuned Results:\\n {results}')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8OT7onOYbne0"
      },
      "source": [
        "## Save, load and test Original vs Manual Fine Tune vs Auto Fine Tune model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OGgtmtgssHzT"
      },
      "source": [
        "### Save models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbvktQVAjygn",
        "outputId": "6c5cceb7-2838-49fa-95dd-34e751b1afd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/task_classification\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# insert the path where you want to save the model\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/task_classification\")\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hL7GXvFxkJd6"
      },
      "outputs": [],
      "source": [
        "### save best manual model to disk\n",
        "directory_save_model = f\"{training_directory}/\"\n",
        "model_name_custom = f\"{model_name.split('/')[-1]}-manual-custom\"\n",
        "manual_mode_custom_path = directory_save_model + model_name_custom\n",
        "\n",
        "# save the model to google drive\n",
        "manual_trainer.save_model(output_dir=manual_mode_custom_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "V14dWwT6q1IU"
      },
      "outputs": [],
      "source": [
        "### save best manual model to disk\n",
        "model_name_custom = f\"{model_name.split('/')[-1]}-auto-custom\"\n",
        "auto_mode_custom_path = directory_save_model + model_name_custom\n",
        "\n",
        "# save the model to google drive\n",
        "auto_trainer.save_model(output_dir=auto_mode_custom_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7TNY5hjRsZCV"
      },
      "source": [
        "### Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zuoIlpzkkSKK"
      },
      "outputs": [],
      "source": [
        "# load your models and tokenizer saved before from disk\n",
        "manual_model = AutoModelForSequenceClassification.from_pretrained(manual_mode_custom_path)\n",
        "manual_tokenizer = AutoTokenizer.from_pretrained(manual_mode_custom_path, use_fast=True, model_max_length=512)  # we load the tokenizer from the original BERT-NLI model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RsQT2DIislPT"
      },
      "outputs": [],
      "source": [
        "# load your models and tokenizer saved before from disk\n",
        "auto_model = AutoModelForSequenceClassification.from_pretrained(auto_mode_custom_path)\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(auto_mode_custom_path, use_fast=True, model_max_length=512)  # we load the tokenizer from the original BERT-NLI model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OTn2gSEPexzd"
      },
      "outputs": [],
      "source": [
        "# create customized_classifier of the fine-tuned model\n",
        "manual_fine_tune_classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=manual_model,  # if you have trained a model above, load_best_model_at_end in the training arguments has automatically replaced model with the fine-tuned model\n",
        "    # or load a model from the Hugging Face hub, e.g. for 0-shot classification\n",
        "    #model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c\",\n",
        "    tokenizer=manual_tokenizer,\n",
        "    framework=\"pt\",\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "\n",
        "auto_fine_tune_classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=auto_model,  # if you have trained a model above, load_best_model_at_end in the training arguments has automatically replaced model with the fine-tuned model\n",
        "    # or load a model from the Hugging Face hub, e.g. for 0-shot classification\n",
        "    #model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c\",\n",
        "    tokenizer=auto_tokenizer,\n",
        "    framework=\"pt\",\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L6fLM0t_v_UU"
      },
      "source": [
        "### Test Original vs Manual Fine Tune vs Auto Fine Tune model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bs2itvpFwDWl"
      },
      "outputs": [],
      "source": [
        "async def task_classifier(client_input: str, task_types: str, classifier_func):\n",
        "    \"\"\"\n",
        "    Classify tasks for LLM-based agent.\n",
        "\n",
        "    Parameters:\n",
        "    - client_input (str): The input text to be classified.\n",
        "    - task_types (str): A comma-separated string of candidate labels for classification.\n",
        "    - classifier_func (callable): The classifier function to be used for classification.\n",
        "\n",
        "    Returns:\n",
        "    A dictionary with the top task type, its score, and the inference time.\n",
        "    \"\"\"\n",
        "    candidate_labels = [label.strip() for label in task_types.split(\",\")]\n",
        "    time_execution_start = time.time()\n",
        "\n",
        "    # Using the passed classifier function\n",
        "    output = classifier_func(str(client_input), candidate_labels, multi_label=False)\n",
        "\n",
        "    time_execution = round(time.time() - time_execution_start, 2)\n",
        "    return {\n",
        "        \"task_type\": output['labels'][0],\n",
        "        \"scores\": round(output['scores'][0], 2),\n",
        "        \"inference_time\": time_execution\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAMDChj3tgI6",
        "outputId": "457d027d-ca18-4bca-973b-d8eae197d477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test case: 'Ein Benutzer bittet ein LLM, einen technischen Artikel vom Deutschen ins Chinesische zu übersetzen.' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.59, 'inference_time': 2.69} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.15, 'inference_time': 0.26} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.15, 'inference_time': 0.21} \n",
            "\n",
            "Test case: 'ユーザーが、英語の文書を日本語に翻訳するようLLMに依頼する。' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.62, 'inference_time': 2.3} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.35, 'inference_time': 0.22} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.35, 'inference_time': 0.21} \n",
            "\n",
            "Test case: 'Пользователь просит LLM перевести техническую статью с русского на английский.' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.56, 'inference_time': 2.23} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.3, 'inference_time': 0.21} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.3, 'inference_time': 0.21} \n",
            "\n",
            "Test case: '用户要求LLM将技术文件从中文翻译成越南语。' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.76, 'inference_time': 2.87} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Summarization', 'scores': 0.45, 'inference_time': 0.3} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Summarization', 'scores': 0.45, 'inference_time': 0.3} \n",
            "\n",
            "Test case: 'Người dùng yêu cầu LLM dịch một bài báo kỹ thuật từ tiếng Anh sang tiếng Việt.' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.75, 'inference_time': 3.31} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Summarization', 'scores': 0.14, 'inference_time': 0.28} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Summarization', 'scores': 0.14, 'inference_time': 0.3} \n",
            "\n",
            "Test case: 'Ein LLM begrüßt einen japanischen Benutzer mit einem herzlichen 'Guten Morgen'.' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Summarization', 'scores': 0.37, 'inference_time': 3.33} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Information retrieval', 'scores': 0.15, 'inference_time': 0.31} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Information retrieval', 'scores': 0.15, 'inference_time': 0.33} \n",
            "\n",
            "Test case: 'LLMは、ロシア語を話すユーザーに対して「こんにちは」と挨拶する。' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Greeting', 'scores': 0.42, 'inference_time': 2.44} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.32, 'inference_time': 0.22} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.32, 'inference_time': 0.22} \n",
            "\n",
            "Test case: 'LLM chào mừng người dùng bằng cách nói 'Xin chào' bằng tiếng Việt.' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.38, 'inference_time': 2.41} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.48, 'inference_time': 0.21} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.48, 'inference_time': 0.2} \n",
            "\n",
            "Test case: 'LLM开始与使用中文的用户进行对话，首先说'你好'。' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Greeting', 'scores': 0.58, 'inference_time': 2.23} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.34, 'inference_time': 0.22} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.34, 'inference_time': 0.22} \n",
            "\n",
            "Test case: 'Пользователь входит в чат, и LLM приветствует его на русском языке.' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Greeting', 'scores': 0.75, 'inference_time': 2.35} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Greeting', 'scores': 0.32, 'inference_time': 0.3} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Greeting', 'scores': 0.32, 'inference_time': 0.27} \n",
            "\n",
            "Test case: 'ユーザーが、最新の量子コンピューティングに関する情報をLLMに問い合わせる。' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.53, 'inference_time': 3.06} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: {'task_type': 'Q&A', 'scores': 0.47, 'inference_time': 0.28} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.47, 'inference_time': 0.26} \n",
            "\n",
            "Test case: 'Ein Benutzer fragt ein LLM nach den neuesten Forschungsergebnissen in der Biotechnologie auf Deutsch.' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.41, 'inference_time': 3.1} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Information retrieval', 'scores': 0.24, 'inference_time': 0.28} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Information retrieval', 'scores': 0.24, 'inference_time': 0.3} \n",
            "\n",
            "Test case: '用户向LLM查询有关可持续能源技术的最新信息。' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Information retrieval', 'scores': 0.79, 'inference_time': 3.11} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Information retrieval', 'scores': 0.52, 'inference_time': 0.35} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Information retrieval', 'scores': 0.52, 'inference_time': 0.35} \n",
            "\n",
            "Test case: 'Người dùng yêu cầu LLM tìm kiếm thông tin về công nghệ AI mới nhất bằng tiếng Việt.' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.52, 'inference_time': 2.69} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.38, 'inference_time': 0.22} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Translation', 'scores': 0.38, 'inference_time': 0.2} \n",
            "\n",
            "Test case: 'Пользователь спрашивает у LLM о последних достижениях в космической отрасли на русском языке.' \n",
            "\n",
            "## Default Model ##\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.77, 'inference_time': 2.48} \n",
            "\n",
            "## Manual Fine-tuned Model ##\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.32, 'inference_time': 0.21} \n",
            "\n",
            "## Auto Fine-tuned Model ##\n",
            "Output: {'task_type': 'Q&A', 'scores': 0.32, 'inference_time': 0.21} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# List of test cases\n",
        "task_types = 'Greeting, Information retrieval, Sentiment analysis, Text generation, Code generation, Q&A, Summarization, Translation'\n",
        "test_cases = [\n",
        "    # Expected LLM Task: Translation\n",
        "    \"Ein Benutzer bittet ein LLM, einen technischen Artikel vom Deutschen ins Chinesische zu übersetzen.\", # A user asks an LLM to translate a technical article from German to Chinese.\n",
        "    \"ユーザーが、英語の文書を日本語に翻訳するようLLMに依頼する。\", # A user requests an LLM to translate an English document into Japanese.\n",
        "    \"Пользователь просит LLM перевести техническую статью с русского на английский.\", # A user asks an LLM to translate a technical article from Russian to English.\n",
        "    \"用户要求LLM将技术文件从中文翻译成越南语。\", # A user requests an LLM to translate a technical document from Chinese to Vietnamese.\n",
        "    \"Người dùng yêu cầu LLM dịch một bài báo kỹ thuật từ tiếng Anh sang tiếng Việt.\", # A user asks an LLM to translate a technical article from English to Vietnamese.\n",
        "\n",
        "    # Expected LLM Task: Greeting\n",
        "    \"Ein LLM begrüßt einen japanischen Benutzer mit einem herzlichen 'Guten Morgen'.\", # An LLM greets a Japanese user with a warm 'Good Morning'.\n",
        "    \"LLMは、ロシア語を話すユーザーに対して「こんにちは」と挨拶する。\", # An LLM greets a Russian-speaking user with 'Hello'.\n",
        "    \"LLM chào mừng người dùng bằng cách nói 'Xin chào' bằng tiếng Việt.\", # An LLM welcomes a user by saying 'Hello' in Vietnamese.\n",
        "    \"LLM开始与使用中文的用户进行对话，首先说'你好'。\", # An LLM begins a conversation with a user in Chinese, starting with 'Hello'.\n",
        "    \"Пользователь входит в чат, и LLM приветствует его на русском языке.\", # A user enters a chat, and the LLM greets them in Russian.\n",
        "\n",
        "    # Expected LLM Task: Information Retrieval\n",
        "    \"ユーザーが、最新の量子コンピューティングに関する情報をLLMに問い合わせる。\", # A user queries an LLM for the latest information on quantum computing in Japanese.\n",
        "    \"Ein Benutzer fragt ein LLM nach den neuesten Forschungsergebnissen in der Biotechnologie auf Deutsch.\", # A user asks an LLM for the latest research findings in biotechnology in German.\n",
        "    \"用户向LLM查询有关可持续能源技术的最新信息。\", # A user queries an LLM for the latest information on sustainable energy technology in Chinese.\n",
        "    \"Người dùng yêu cầu LLM tìm kiếm thông tin về công nghệ AI mới nhất bằng tiếng Việt.\", # A user asks an LLM to search for the latest information on AI technology in Vietnamese.\n",
        "    \"Пользователь спрашивает у LLM о последних достижениях в космической отрасли на русском языке.\", # A user inquires with an LLM about the latest advancements in the space industry in Russian.\n",
        "]\n",
        "\n",
        "# Run the task_classifier function for each test case\n",
        "for test_case in test_cases:\n",
        "    print(f\"## Test case: '{test_case}' \\n\")\n",
        "    print(f\"# Default Model \")\n",
        "    result = await task_classifier(test_case, task_types, classifier)\n",
        "    print(f\"Output: {result} \\n\")\n",
        "    print(f\"# Manual Fine-tuned Model ##\")\n",
        "    result = await task_classifier(test_case, task_types, manual_fine_tune_classifier)\n",
        "    print(f\"Output: {result} \\n\")\n",
        "    print(f\"# Auto Fine-tuned Model ##\")\n",
        "    result = await task_classifier(test_case, task_types, auto_fine_tune_classifier)\n",
        "    print(f\"Output: {result} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0i8eE1nkeDn",
        "outputId": "ae4ccfd5-846e-4f40-ed7f-6787770386d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default Model: task_type: Summarization, scores: 0.33, inference_time: 7.01\n",
            "Manual Fine-tuned Model: task_type: Conversation and Chatbots, scores: 0.53, inference_time: 0.53\n",
            "Auto Fine-tuned Model: task_type: Conversation and Chatbots, scores: 0.53, inference_time: 0.54\n"
          ]
        }
      ],
      "source": [
        "#experiments\n",
        "input = \"An LLM functions as a virtual assistant for legal advice, requiring it to understand and respond accurately to complex legal queries while maintaining conversational flow.\"\n",
        "candidate_labels = [\"Greeting\", \"Information retrieval\", \"Sentiment analysis\", \"Text generation\", \"Code generation\", \"Q&A\", \"Summarization\", \"Translation\", \"Conversation and Chatbots\"]\n",
        "\n",
        "# Default model\n",
        "time_execution = time.time()\n",
        "output = classifier(input, candidate_labels, multi_label=False)\n",
        "time_execution = round(time.time() - time_execution, 2)\n",
        "print(f\"Default Model: task_type: {output['labels'][0]}, scores: {round(output['scores'][0],2)}, inference_time: {time_execution}\")\n",
        "\n",
        "# Manual Fine-tuned Model\n",
        "time_execution = time.time()\n",
        "output = manual_fine_tune_classifier(input, candidate_labels, multi_label=False)\n",
        "time_execution = round(time.time() - time_execution, 2)\n",
        "print(f\"Manual Fine-tuned Model: task_type: {output['labels'][0]}, scores: {round(output['scores'][0],2)}, inference_time: {time_execution}\")\n",
        "\n",
        "# Auto Fine-tuned Model\n",
        "time_execution = time.time()\n",
        "output = auto_fine_tune_classifier(input, candidate_labels, multi_label=False)\n",
        "time_execution = round(time.time() - time_execution, 2)\n",
        "print(f\"Auto Fine-tuned Model: task_type: {output['labels'][0]}, scores: {round(output['scores'][0],2)}, inference_time: {time_execution}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d12e5fbbc3740239a556903e2149ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea9281837f6f4abbbd4114206058d6a8",
              "IPY_MODEL_3d515140dfc843b4af7ccc01690a2c50",
              "IPY_MODEL_6a55388a041e4cc79edb4cb097e66eda"
            ],
            "layout": "IPY_MODEL_8c1eb20063374289ac6637c958653239"
          }
        },
        "2603e3b46062490c8f5fabf01910ee9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37bf329a11a84e6085299faadbc90e17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d515140dfc843b4af7ccc01690a2c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a382ddde21ed4ce89afa932cb14fd4e6",
            "max": 195,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e4279c518304621a815875b339dc928",
            "value": 195
          }
        },
        "5625b80df03e4ab19dd970f983b04777": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_961a092682324bbb9b664e410d50ea0f",
              "IPY_MODEL_802d91d2d17f440cb65b4e81e4db5efe",
              "IPY_MODEL_c020e1635c2d48e698ab28f143605436"
            ],
            "layout": "IPY_MODEL_d840e2c9d0fe4284b7b1b3ae9b4b3c99"
          }
        },
        "6a55388a041e4cc79edb4cb097e66eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c4c0a98b37640cbb90b2a389b978396",
            "placeholder": "​",
            "style": "IPY_MODEL_2603e3b46062490c8f5fabf01910ee9c",
            "value": " 195/195 [00:00&lt;00:00, 700.16 examples/s]"
          }
        },
        "71f7703db6ec4b8cbecb339be23a94c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802d91d2d17f440cb65b4e81e4db5efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e88e528d3e4a46119f012503af71bb9a",
            "max": 453,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cca7064fd0bd4a57bc20b8913bc7d13d",
            "value": 453
          }
        },
        "8c1eb20063374289ac6637c958653239": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c69739cc36a4663868ffc8ee7a4a016": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "961a092682324bbb9b664e410d50ea0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37bf329a11a84e6085299faadbc90e17",
            "placeholder": "​",
            "style": "IPY_MODEL_eff7b9dac98d46a0b4c80b2a4ef027f1",
            "value": "Map: 100%"
          }
        },
        "9c4c0a98b37640cbb90b2a389b978396": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e4279c518304621a815875b339dc928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a32f69f0e59e4108b8daf42c0c273d27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a382ddde21ed4ce89afa932cb14fd4e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8442770919d46b1a3ae5bd0fbeaede3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c020e1635c2d48e698ab28f143605436": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71f7703db6ec4b8cbecb339be23a94c1",
            "placeholder": "​",
            "style": "IPY_MODEL_b8442770919d46b1a3ae5bd0fbeaede3",
            "value": " 453/453 [00:00&lt;00:00, 1185.56 examples/s]"
          }
        },
        "cca7064fd0bd4a57bc20b8913bc7d13d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d840e2c9d0fe4284b7b1b3ae9b4b3c99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88e528d3e4a46119f012503af71bb9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea9281837f6f4abbbd4114206058d6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a32f69f0e59e4108b8daf42c0c273d27",
            "placeholder": "​",
            "style": "IPY_MODEL_8c69739cc36a4663868ffc8ee7a4a016",
            "value": "Map: 100%"
          }
        },
        "eff7b9dac98d46a0b4c80b2a4ef027f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
