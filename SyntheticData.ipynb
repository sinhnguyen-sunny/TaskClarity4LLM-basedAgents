{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "szqyByaYML8F"
      },
      "source": [
        "# 0. Installation\n",
        "\n",
        "Install the SDV library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEBBEicLMJaQ",
        "outputId": "13eed312-a0ea-4f95-f8a7-25d0b5dba7ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==1.2.4 in /usr/local/lib/python3.10/dist-packages (1.2.4)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.4) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.2.4) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.4) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.4) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.4) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.4) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.4) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.4) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.4) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.2.4) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.2.4) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai==1.2.4) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.2.4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K2T_zjORB50i"
      },
      "source": [
        "# 2. Script for generating Data from Sample Output CSV and Prompt Instruction\n",
        "### Script for Generating Balanced LLM NLI Dataset\n",
        "\n",
        "#### Overview\n",
        "- **Purpose**: Automates the creation of a Balanced Dataset using OpenAI's GPT model.\n",
        "\n",
        "#### Input\n",
        "- **Sample CSV**: Path to a sample CSV file for understanding data structure.\n",
        "- **User-Defined Prompt**: Custom prompt instruction provided by the user.\n",
        "- **Configuration**:\n",
        "  - `total_rows_target`: Target number of rows to generate.\n",
        "  - `max_tokens_per_call`: Maximum token limit for each GPT API call.\n",
        "\n",
        "#### Process\n",
        "1. **Read Sample CSV**: Analyzes the sample CSV to define the data structure.\n",
        "2. **Generate Prompt**: Merges the user-defined prompt with examples from the CSV to guide the GPT model.\n",
        "3. **Data Generation**:\n",
        "   - Iteratively calls the GPT API to generate data.\n",
        "   - Performs quality checks on generated entries for uniqueness and relevance.\n",
        "\n",
        "#### Output\n",
        "- **Generated Dataset**: A Balanced Dataset, tailored to the specific structure and context defined by the user and sample CSV.\n",
        "\n",
        "#### Goal\n",
        "- Efficient creation of large, diverse, and balanced datasets, leveraging the advanced capabilities of GPT for specific data generation needs.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO7PbNgfBlwA",
        "outputId": "a2c32784-4a2f-4ab2-8bd7-ff8c766d074d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6zNF7DWBs3n",
        "outputId": "d732e972-99c1-4d0e-f08a-3a3d6dc9a92e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/1000 [00:15<4:16:58, 15.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 2/1000 [00:27<3:41:53, 13.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 43\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 3/1000 [00:38<3:24:40, 12.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 4/1000 [00:51<3:30:18, 12.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 73\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 5/1000 [01:03<3:23:14, 12.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 91\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 6/1000 [01:16<3:28:24, 12.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 7/1000 [01:32<3:45:04, 13.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 8/1000 [01:43<3:35:48, 13.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 9/1000 [01:57<3:38:49, 13.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 178\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 10/1000 [02:10<3:34:50, 13.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 194\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 11/1000 [02:21<3:25:56, 12.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 12/1000 [02:32<3:16:56, 11.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|▏         | 13/1000 [02:41<3:02:29, 11.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|▏         | 14/1000 [02:54<3:10:31, 11.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 15/1000 [03:05<3:09:38, 11.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 16/1000 [03:17<3:14:09, 11.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 17/1000 [03:31<3:22:03, 12.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 319\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 18/1000 [03:47<3:42:10, 13.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 343\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 19/1000 [04:00<3:39:20, 13.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 20/1000 [04:12<3:30:27, 12.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 388\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 21/1000 [04:23<3:22:19, 12.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 22/1000 [04:35<3:16:34, 12.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 23/1000 [04:51<3:35:25, 13.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 24/1000 [05:06<3:47:17, 13.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▎         | 25/1000 [05:19<3:42:05, 13.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 26/1000 [05:32<3:35:25, 13.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 503\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 27/1000 [05:48<3:48:28, 14.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 524\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 28/1000 [06:05<4:03:04, 15.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 29/1000 [06:17<3:49:49, 14.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 30/1000 [06:31<3:47:59, 14.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 593\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 31/1000 [06:44<3:40:21, 13.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 32/1000 [06:58<3:42:35, 13.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 33/1000 [07:10<3:35:30, 13.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 34/1000 [07:22<3:30:13, 13.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▎         | 35/1000 [07:36<3:33:38, 13.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▎         | 36/1000 [07:50<3:38:11, 13.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 710\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▎         | 37/1000 [08:01<3:23:09, 12.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 731\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 38/1000 [08:16<3:35:08, 13.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 752\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 39/1000 [08:31<3:40:00, 13.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 40/1000 [08:43<3:35:01, 13.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 41/1000 [08:58<3:38:32, 13.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 42/1000 [09:07<3:20:11, 12.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 43/1000 [09:20<3:18:08, 12.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 44/1000 [09:34<3:26:18, 12.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 45/1000 [09:48<3:31:14, 13.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▍         | 46/1000 [10:04<3:45:37, 14.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▍         | 47/1000 [10:17<3:38:27, 13.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 940\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▍         | 48/1000 [10:34<3:54:20, 14.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 961\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▍         | 49/1000 [10:48<3:49:55, 14.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 50/1000 [11:01<3:44:49, 14.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 50/1000 [11:13<3:33:21, 13.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current number of generated rows: 1020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import csv\n",
        "from io import StringIO\n",
        "\n",
        "# Set your OpenAI API key\n",
        "from google.colab import userdata\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=open_ai_key,\n",
        ")\n",
        "\n",
        "def generate_prompt_with_examples(sample_csv_path, user_prompt, num_examples=5):\n",
        "    # Load the sample output CSV file and extract example entries\n",
        "    sample_output_structure = pd.read_csv(sample_csv_path)\n",
        "    output_columns = sample_output_structure.columns.tolist()\n",
        "    example_entries = sample_output_structure.head(num_examples).to_csv(index=False, sep=';', header=False)\n",
        "\n",
        "    # Combine user prompt with examples\n",
        "    prompt_instruction = f\"{user_prompt}\\nHere are some examples:\\n{example_entries}\\nGenerate more entries in CSV format with the following columns: {', '.join(output_columns)}. Use semicolon ';' as the delimiter.\"\n",
        "    return prompt_instruction, output_columns\n",
        "\n",
        "def generate_data(prompt_instruction, max_tokens, output_columns):\n",
        "    generated_entries = []\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_instruction}],\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "\n",
        "    generated_content = response.choices[0].message.content.strip()\n",
        "    reader = csv.reader(StringIO(generated_content), delimiter=';')\n",
        "    generated_entries = [row for row in reader if len(row) == len(output_columns)]\n",
        "\n",
        "    return generated_entries\n",
        "\n",
        "def quality_checks(entries, unique_entries_set):\n",
        "    quality_entries = []\n",
        "    for entry in entries:\n",
        "        entry_tuple = tuple(entry)\n",
        "        if entry_tuple not in unique_entries_set:\n",
        "            quality_entries.append(entry)\n",
        "            unique_entries_set.add(entry_tuple)\n",
        "    return quality_entries\n",
        "\n",
        "# User inputs\n",
        "## Your custom sample .csv file here\n",
        "sample_csv_path = '/content/drive/MyDrive/Colab Notebooks/task_classification/NLI_Task_Classification_Dataset.csv'\n",
        "## Your custom prompt instruction here\n",
        "user_prompt = \"\"\"\n",
        "Create a dataset designed for large language model (LLM) task classification within the Natural Language Inference (NLI) framework. Each entry in the dataset should consist of the following elements:\n",
        "1. **Premise:** Develop a statement that describes a realistic scenario where a specific task of a large language model (LLM) might be applied. Focus on clearly identifiable LLM tasks such as 'Text Generation', 'Conversation and Chatbots', 'Question Answering', 'Content Curation and Recommendation', and others. Ensure the scenarios are varied and accurately represent the diverse range of tasks LLMs are capable of performing.\n",
        "2. **Hypothesis:** For each premise, formulate a statement that has one of the following relationships with the premise:\n",
        "   - Entailment: The hypothesis should logically follow the premise and directly relate to the LLM task described in the premise.\n",
        "   - Neutral: The hypothesis is unrelated or neutral to the premise, not directly commenting on the LLM task described.\n",
        "   - Contradiction: The hypothesis contradicts the premise, suggesting a different LLM task or an outcome inconsistent with the task described in the premise.\n",
        "3. **Label:** Assign one of three labels to each premise-hypothesis pair, indicating the relationship type (entailment, neutral, or contradiction).\n",
        "Please generate a comprehensive and diverse dataset, focused on explicit LLM tasks, to be used for accurate NLI task classification.\n",
        "\"\"\"\n",
        "\n",
        "total_rows_target = 1000 #expected number of generated rows\n",
        "max_tokens_per_call = 3000 #max token for the model\n",
        "\n",
        "# Prepare the prompt with examples\n",
        "prompt_instruction, output_columns = generate_prompt_with_examples(sample_csv_path, user_prompt)\n",
        "unique_entries_set = set()\n",
        "all_generated_data = []\n",
        "\n",
        "for _ in tqdm(range(0, total_rows_target)):\n",
        "    generated_batch = generate_data(prompt_instruction, max_tokens_per_call, output_columns)\n",
        "    quality_batch = quality_checks(generated_batch, unique_entries_set)\n",
        "    all_generated_data.extend(quality_batch)\n",
        "\n",
        "    print(f'Current number of generated rows: {len(all_generated_data)}')\n",
        "    if len(all_generated_data) >= total_rows_target:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aQJSY7f13UTV"
      },
      "outputs": [],
      "source": [
        "# from openai import OpenAI\n",
        "# import pandas as pd\n",
        "# from tqdm import tqdm\n",
        "# import time\n",
        "# import csv\n",
        "# from io import StringIO\n",
        "\n",
        "# # Set your OpenAI API key\n",
        "# from google.colab import userdata\n",
        "# open_ai_key = userdata.get('open_ai_key')\n",
        "\n",
        "# client = OpenAI(\n",
        "#     # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "#     api_key=open_ai_key,\n",
        "# )\n",
        "\n",
        "# # Load the structure from the sample output CSV file\n",
        "# sample_output_structure = pd.read_csv('/content/customer_data/Expanded_LLM_Task_Classification_Dataset.csv')\n",
        "# output_columns = sample_output_structure.columns.tolist()\n",
        "\n",
        "# #Define your prompt instruction\n",
        "# prompt_instruction = \"\"\"Create a dataset for task classification with entries in Natural Language Inference (NLI) format. Each entry should consist of:\n",
        "# * A Premise: This should be a statement describing a realistic scenario in which a popular task of an LLM might be used, such as language translation, summarization, content generation, question answering, etc.\n",
        "# * A Hypothesis: This should be a statement that either follows logically from the premise (entailment), is unrelated or neutral to the premise (neutral), or contradicts the premise (contradiction).\n",
        "# * A NLILabel: This should be one of the three NLI labels ('entailment', 'neutral', 'contradiction') based on the relationship between the premise and the hypothesis.\n",
        "# * A Task: This should be tasks performed by LLMs, ex: 'Text Generation', 'Conversation and Chatbots', 'Question Answering', 'Content Curation and Recommendation', etc..\n",
        "# The dataset should reflect tasks performed by LLMs, ensuring that each entry's hypothesis is a realistic outcome or expectation of the premise described. Please balance the dataset with an equal number of entries for each label. Generate entries to form a diverse dataset that encompasses various domains where LLMs are typically applied.\n",
        "# \"\"\"\n",
        "\n",
        "# prompt_instruction = \"\"\"Create a dataset for task classification with entries in Natural Language Inference (NLI) format. Each entry should consist of:\n",
        "# * A Premise: This should be a statement describing a realistic scenario in which a popular task of an LLM might be used, such as language translation, summarization, content generation, question answering, etc.\n",
        "# * A Hypothesis: This should be a statement that either follows logically from the premise (entailment), is unrelated or neutral to the premise (neutral), or contradicts the premise (contradiction).\n",
        "# * A Label: This should be one of the three NLI labels ('entailment', 'neutral', 'contradiction') based on the relationship between the premise and the hypothesis.\n",
        "# The dataset should reflect tasks performed by LLMs, ensuring that each entry's hypothesis is a realistic outcome or expectation of the premise described. Please balance the dataset with an equal number of entries for each label. Generate entries to form a diverse dataset that encompasses various domains where LLMs are typically applied.\n",
        "# \"\"\"\n",
        "# #Fix the Output Format which is generated from OpenAI Models\n",
        "# prompt_instruction = f\"Generate data in CSV format with the following columns: {', '.join(output_columns)}. Use semicolon ';' as the delimiter. \" + prompt_instruction\n",
        "\n",
        "\n",
        "# # Desired number of total rows and max tokens per API call\n",
        "# total_rows_target = 500\n",
        "# max_tokens_per_call = 3000  # Maximum tokens for GPT-3.5 Turbo\n",
        "\n",
        "# # Function to generate data using GPT-3.5 Turbo\n",
        "# def generate_data(prompt_instruction, max_tokens, output_format):\n",
        "#     generated_entries = []\n",
        "\n",
        "#     customized_prompt = f\"{prompt_instruction} Generate output in the format: {output_format}\"\n",
        "#     response = client.chat.completions.create(\n",
        "#     model=\"gpt-3.5-turbo\",\n",
        "#     messages=[{\"role\": \"user\", \"content\": customized_prompt}],\n",
        "#     max_tokens= max_tokens\n",
        "#     )\n",
        "#     generated_content = response.choices[0].message.content\n",
        "#     reader = csv.reader(StringIO(generated_content), delimiter=';')\n",
        "#     generated_entries = [row for row in reader if len(row) == len(output_columns)]\n",
        "\n",
        "#     return generated_entries\n",
        "\n",
        "# # Set for storing unique entries (as tuples for comparison)\n",
        "# unique_entries_set = set()\n",
        "\n",
        "# # Function for quality checks including deduplication\n",
        "# def quality_checks(entries, unique_entries_set):\n",
        "#     quality_entries = []\n",
        "#     for entry in entries:\n",
        "#         entry_tuple = tuple(entry)\n",
        "#         if entry_tuple not in unique_entries_set:\n",
        "#             quality_entries.append(entry)\n",
        "#             unique_entries_set.add(entry_tuple)\n",
        "#     return quality_entries\n",
        "\n",
        "# # Main loop for batch processing\n",
        "# all_generated_data = []\n",
        "\n",
        "# for _ in tqdm(range(0, total_rows_target)):\n",
        "#     generated_batch = generate_data(prompt_instruction, max_tokens_per_call, ';'.join(output_columns))\n",
        "#     quality_batch = quality_checks(generated_batch, unique_entries_set)\n",
        "#     all_generated_data.extend(quality_batch)\n",
        "#     print(f'Current number of generated rows: {len(all_generated_data)}')\n",
        "\n",
        "#     if len(all_generated_data) >= total_rows_target:\n",
        "#         break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyrTS5NUXFQG",
        "outputId": "589ce6a5-e572-4cf9-a385-f82d8f4f075a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data generation complete.\n"
          ]
        }
      ],
      "source": [
        "# Convert the list of dictionaries to a DataFrame with specified column names\n",
        "df = pd.DataFrame(all_generated_data, columns=output_columns)\n",
        "\n",
        "# Save DataFrame to CSV without index and with proper header\n",
        "df.to_csv(\"/content/LLM_Task_NLI_Dataset.csv\", index=False)\n",
        "\n",
        "print(\"Data generation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lvh0epofDQlz",
        "outputId": "c68c9c5a-51ce-46db-9029-35a6c67f62cf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d24f21bd-c093-4e9f-83ed-4b995a1618cf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>Generate short descriptions for online product...</td>\n",
              "      <td>The output will be a sales report.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>Generate short descriptions for online product...</td>\n",
              "      <td>This is a machine translation task.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>Compose an email to a friend based on a given ...</td>\n",
              "      <td>This is a text generation task.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>Compose an email to a friend based on a given ...</td>\n",
              "      <td>The output will be a song lyrics.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>Compose an email to a friend based on a given ...</td>\n",
              "      <td>This is a sentiment analysis task.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d24f21bd-c093-4e9f-83ed-4b995a1618cf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d24f21bd-c093-4e9f-83ed-4b995a1618cf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d24f21bd-c093-4e9f-83ed-4b995a1618cf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ca36f7f0-5b46-4bac-8450-c297d67e16cb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ca36f7f0-5b46-4bac-8450-c297d67e16cb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ca36f7f0-5b46-4bac-8450-c297d67e16cb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                      0  \\\n",
              "1015  Generate short descriptions for online product...   \n",
              "1016  Generate short descriptions for online product...   \n",
              "1017  Compose an email to a friend based on a given ...   \n",
              "1018  Compose an email to a friend based on a given ...   \n",
              "1019  Compose an email to a friend based on a given ...   \n",
              "\n",
              "                                        1              2  \n",
              "1015   The output will be a sales report.  contradiction  \n",
              "1016  This is a machine translation task.  contradiction  \n",
              "1017      This is a text generation task.     entailment  \n",
              "1018    The output will be a song lyrics.  contradiction  \n",
              "1019   This is a sentiment analysis task.  contradiction  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(all_generated_data).tail()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
